{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e6fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56eb83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member:\n",
    "    def __init__(self, r_d, label=None, doc_id=None):\n",
    "        self._r_d = r_d\n",
    "        self._label = label\n",
    "        self._doc_id = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d43d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self._centroid = None\n",
    "        self._members = []\n",
    "        \n",
    "    def reset_members(self):\n",
    "        self._members = []\n",
    "        \n",
    "    def add_member(self, member):\n",
    "        self._members.append(member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f19445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    def __init__(self, num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = [Cluster() for _ in range(self._num_clusters)]\n",
    "        self._E = [] # list of centroids\n",
    "        self._S = 0 # overall similarity\n",
    "        \n",
    "    def load_data(self, data_path):\n",
    "        def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "            r_d = [0.0 for _ in range(vocab_size)]\n",
    "            indices_tfidfs = sparse_r_d.split()\n",
    "            for index_tfidf in indices_tfidfs:\n",
    "                index = int(index_tfidf.split(\":\")[0])\n",
    "                tfidf = float(index_tfidf.split(\":\")[1])\n",
    "                r_d[index] = tfidf\n",
    "            return np.array(r_d)\n",
    "        \n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        with open(os.getcwd()+'/20news-bydate/20news-full-words-idfs.txt') as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "        \n",
    "        self._data = []\n",
    "        self._label_count = defaultdict(int)\n",
    "        for data_id, d in enumerate(d_lines):\n",
    "            features = d.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            self._label_count[label] += 1\n",
    "            r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
    "            self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
    "    \n",
    "    def random_init(self, seed_value):\n",
    "        set_candidates = set(range(len(self._data)))\n",
    "        set_candidates.remove(seed_value)\n",
    "        E = [self._data[seed_value]._r_d]\n",
    "        while len(E) < self._num_clusters:\n",
    "            new_centroid_id = None\n",
    "            min_similarity_val = 1\n",
    "            for i in set_candidates:\n",
    "                local_max_similarity = -1\n",
    "                for centroid in E:\n",
    "                    local_max_similarity = max(local_max_similarity, self.compute_similarity(self._data[i], centroid))\n",
    "                if local_max_similarity < min_similarity_val:\n",
    "                    new_centroid_id = i\n",
    "                    min_similarity_val = local_max_similarity\n",
    "            if new_centroid_id:\n",
    "                set_candidates.remove(new_centroid_id)\n",
    "                E.append(self._data[new_centroid_id]._r_d)\n",
    "            else:\n",
    "                raise Exeption(\"Could not find enough centroids\")\n",
    "        for i in range(len(self._clusters)):\n",
    "            self._clusters[i]._centroid = E[i]\n",
    "\n",
    "    def compute_similarity(self, member, centroid):\n",
    "        return np.dot(member._r_d, centroid)/(np.linalg.norm(member._r_d)*np.linalg.norm(centroid))\n",
    "    \n",
    "    def select_cluster_for(self, member):\n",
    "        best_fit_cluster = None\n",
    "        max_similarity = -1\n",
    "        for cluster in self._clusters:\n",
    "            if self.compute_similarity(member, cluster._centroid) > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = self.compute_similarity(member, cluster._centroid)\n",
    "        \n",
    "        best_fit_cluster.add_member(member)\n",
    "        return max_similarity\n",
    "    \n",
    "    def update_centroid_of(self, cluster):\n",
    "        member_r_ds = [member._r_d for member in cluster._members]\n",
    "        aver_r_d = np.mean(member_r_ds, axis = 0)\n",
    "        new_centroid = aver_r_d/np.linalg.norm(aver_r_d)\n",
    "        cluster._centroid = new_centroid\n",
    "        return cluster._centroid\n",
    "    \n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        criteria = ['centroid', 'similarity', 'max_iters']\n",
    "        assert criterion in criteria\n",
    "        if criterion == 'max_iters':\n",
    "            if self._iteration >= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif criterion == 'centroid':\n",
    "            E_new = [list(cluster._centroid) for cluster in self._clusters]\n",
    "            E_new_minus_E = [centroid for centroid in E_new if centroid not in self._E]\n",
    "            self._E = E_new\n",
    "            if len(E_new_minus_E) <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            new_S_minus_S = self._new_S - self._S\n",
    "            self._S = self._new_S\n",
    "            if new_S_minus_S <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def run(self, seed_value, criterion, threshold):\n",
    "        self.random_init(seed_value)\n",
    "        \n",
    "        # continually update clusters until convergence\n",
    "        self._iteration = 0\n",
    "        while True:\n",
    "            # reset clusters, retain only centroids\n",
    "            for cluster in self._clusters:\n",
    "                cluster.reset_members()\n",
    "            self._new_S = 0\n",
    "            for member in self._data:\n",
    "                max_s = self.select_cluster_for(member)\n",
    "                self._new_S += max_s\n",
    "            for cluster in self._clusters:\n",
    "                self.update_centroid_of(cluster)\n",
    "                \n",
    "            self._iteration += 1\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                break\n",
    "                \n",
    "    def compute_purity(self):\n",
    "        majority_sum = 0\n",
    "        for cluster in self._clusters:\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            max_count = max([member_labels.count(label) for label in range(num_clusters)])\n",
    "            majority_sum += max_count\n",
    "        return majority_sum * 1 / len(self._data)\n",
    "    \n",
    "    def compute_NMI(self):\n",
    "        I_value, H_O, H_C, N = 0.0, 0.0, 0.0, len(self._data)\n",
    "\n",
    "        for cluster in self._clusters:\n",
    "            wk = len(cluster._members) * 1.0\n",
    "            H_O += -wk / N * np.log10(wk / N)\n",
    "\n",
    "        for label in range(num_clusters):\n",
    "            cj = self._label_count[label] * 1.0\n",
    "            H_C += -cj / N * np.log10(cj / N)\n",
    "\n",
    "        for cluster in self._clusters:\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            for label in range(num_clusters):\n",
    "                wk_cj = member_labels.count(label) * 1.0\n",
    "                wk = len(cluster._members) * 1.0\n",
    "                cj = self._label_count[label] * 1.0\n",
    "                I_value += wk_cj / N * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
    "\n",
    "        return I_value * 2.0 / (H_C + H_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f751269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kmeans sample\n",
    "num_clusters = 20\n",
    "k = Kmeans(num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fcaf9b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9165894",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.load_data(os.getcwd()+\"/20news-bydate/20news-full-tf-idf.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84baae69",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6c3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given number clusters: 20\n",
      "Number clusters: 20\n",
      "Label dictionary: defaultdict(<class 'int'>, {0: 799, 1: 973, 2: 985, 3: 982, 4: 963, 5: 988, 6: 975, 7: 990, 8: 996, 9: 994, 10: 999, 11: 991, 12: 984, 13: 990, 14: 987, 15: 997, 16: 910, 17: 940, 18: 775, 19: 628})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Given number clusters: {num_clusters}\")\n",
    "print(f\"Number clusters: {len(k._label_count)}\")\n",
    "print(f\"Label dictionary: {k._label_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bc87a",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f266305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.run(seed_value=np.random.choice(range(len(k._data))), criterion=\"centroid\", threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32f5e",
   "metadata": {},
   "source": [
    "## Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2279bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.5343308924970817\n",
      "NMI: 0.5435565697667284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Purity: {k.compute_purity()}\")\n",
    "print(f\"NMI: {k.compute_NMI()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

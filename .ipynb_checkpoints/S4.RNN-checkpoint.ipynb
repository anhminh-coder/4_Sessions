{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1f7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f258363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = parent_path + '\\\\' + newsgroup +'\\\\'\n",
    "            files = [(filename, dir_path + filename) for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
    "            files.sort()\n",
    "            label = group_id\n",
    "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
    "            \n",
    "            for filename, filepath in files:\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            word_count[word] += 1\n",
    "                    content = ' '.join(words)\n",
    "                    assert len(content.splitlines()) == 1\n",
    "                    data.append(str(label) + '<fff>' + filename +'<fff>' +content)\n",
    "        return data\n",
    "        \n",
    "    word_count = defaultdict(int)\n",
    "    \n",
    "    path = os.getcwd()+'/20news-bydate/'\n",
    "    parts = [path + dir_name + '\\\\' for dir_name in os.listdir(path) if not os.path.isfile(path + dir_name)]\n",
    "    \n",
    "    train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
    "    \n",
    "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    \n",
    "    train_data = collect_data_from(\n",
    "        parent_path = train_path,\n",
    "        newsgroup_list = newsgroup_list,\n",
    "        word_count = word_count\n",
    "    )\n",
    "    \n",
    "    vocab = [word for word, freq in zip(word_count.keys(), word_count.values()) if freq > 10]\n",
    "    vocab.sort()\n",
    "    with open(os.getcwd()+'/w2v/vocab-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    \n",
    "    test_data = collect_data_from(\n",
    "        parent_path = test_path,\n",
    "        newsgroup_list = newsgroup_list\n",
    "    )\n",
    "    \n",
    "    with open(os.getcwd()+'/w2v/20news-train-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    \n",
    "    with open(os.getcwd()+'/w2v/20news-test-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55273a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID + 2) for word_ID, word in enumerate(f.read().splitlines())])\n",
    "        \n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    encoded_data = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_DOC_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(unknown_ID))\n",
    "                \n",
    "        if len(words) < MAX_DOC_LENGTH:\n",
    "            num_padding = MAX_DOC_LENGTH - len(words)\n",
    "            for _ in range(num_padding):\n",
    "                encoded_text.append(str(padding_ID))\n",
    "        \n",
    "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
    "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
    "    file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "    with open(dir_name + '/' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e45d5b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ad38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_ID = 0\n",
    "padding_ID = 1\n",
    "MAX_DOC_LENGTH = 500\n",
    "train_data_path = os.getcwd()+'/w2v/20news-train-raw.txt'\n",
    "test_data_path = os.getcwd()+'/w2v/20news-test-raw.txt'\n",
    "vocab_path = os.getcwd()+'/w2v/vocab-raw.txt'\n",
    "encode_data(train_data_path, vocab_path)\n",
    "encode_data(test_data_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c5c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self, data, labels, sentence_lengths):\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._sentence_lengths = sentence_lengths\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self._batch_id * batch_size\n",
    "        end = start + batch_size\n",
    "        self._batch_id += 1\n",
    "        if end + batch_size > len(self._data):\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            np.random.seed(2020)\n",
    "            np.random.shuffle(indices)\n",
    "            self._data, self._labels, self._sentence_lengths = self._data[indices], self._labels[indices], self._sentence_lengths[indices]\n",
    "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]\n",
    "\n",
    "    def load_data(data_path):\n",
    "        with open(data_path, encoding = 'latin1') as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        data, labels, sentence_lengths = [], [], []\n",
    "        for line in d_lines:\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id, sentence_len = int(features[0]), int(features[1]), int(features[2])\n",
    "            vector = [int(ID) for ID in features[3].split()]\n",
    "            data.append(vector)\n",
    "            labels.append(label)\n",
    "            sentence_lengths.append(sentence_len)\n",
    "        return np.array(data), np.array(labels), np.array(sentence_lengths)\n",
    "  \n",
    "    train_data, train_labels, train_sentence_lengths = load_data(\n",
    "        data_path=os.getcwd()+'/w2v/20news-train-encoded.txt'\n",
    "    )\n",
    "    test_data, test_labels, test_sentence_lengths = load_data(\n",
    "        data_path=os.getcwd()+'/w2v/20news-test-encoded.txt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0020d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import numpy as np\n",
    "\n",
    "MAX_DOC_LENGTH = 500\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "class RNN:\n",
    "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._embedding_size = embedding_size\n",
    "    self._lstm_size = lstm_size\n",
    "    self._batch_size = batch_size\n",
    "\n",
    "    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
    "    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "\n",
    "  def embedding_layer(self, indices):\n",
    "    pretrained_vectors = []\n",
    "    pretrained_vectors.append(np.zeros(self._embedding_size))\n",
    "    for _ in range(self._vocab_size + 1):\n",
    "      pretrained_vectors.append(np.random.normal(loc=0, scale=1, size=self._embedding_size))\n",
    "\n",
    "    pretrained_vectors = np.array(pretrained_vectors)\n",
    "    self._embedding_matrix = tf.get_variable(\n",
    "        name='embedding',\n",
    "        shape=(self._vocab_size + 2,self._embedding_size),\n",
    "        initializer=tf.constant_initializer(pretrained_vectors)\n",
    "    )\n",
    "    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
    "\n",
    "  def LSTM_layer(self, embeddings):\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
    "    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
    "    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "\n",
    "    lstm_inputs = tf.unstack(\n",
    "        tf.transpose(embeddings, perm=[1,0,2])\n",
    "    )\n",
    "    lstm_outputs, last_state = tf.nn.static_rnn(\n",
    "        cell=lstm_cell,\n",
    "        inputs=lstm_inputs,\n",
    "        initial_state=initial_state,\n",
    "        sequence_length=self._sentence_lengths\n",
    "    )# [num_docs, lstm_size]\n",
    "    lstm_outputs = tf.unstack(\n",
    "        tf.transpose(lstm_outputs, perm=[1,0,2])\n",
    "    )\n",
    "    lstm_outputs = tf.concat(\n",
    "        lstm_outputs, axis=0\n",
    "    )# [num_docs*MAX_SENT_LENGTH, lstm_size]\n",
    "    mask = tf.sequence_mask(\n",
    "        lengths=self._sentence_lengths,\n",
    "        maxlen=MAX_DOC_LENGTH,\n",
    "        dtype=tf.float32\n",
    "    )# [num_docs, MAX_SENTENCE_LENGTH]\n",
    "    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
    "    mask = tf.expand_dims(mask, -1)\n",
    "    lstm_outputs = mask * lstm_outputs\n",
    "    lstm_outputs_split = tf.split(lstm_outputs, \n",
    "                                  num_or_size_splits=self._batch_size)\n",
    "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)#[num_docs, lstm_size]\n",
    "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
    "        tf.cast(self._sentence_lengths, tf.float32), -1)#[num_docs, lstm_size]\n",
    "    return lstm_outputs_average\n",
    "\n",
    "  def build_graph(self):\n",
    "    embeddings = self.embedding_layer(self._data)\n",
    "    lstm_outputs = self.LSTM_layer(embeddings)\n",
    "\n",
    "    weights = tf.get_variable(\n",
    "        name='final_layer_weights',\n",
    "        shape=(self._lstm_size, NUM_CLASSES),\n",
    "        initializer=tf.random_normal_initializer(seed=2020)\n",
    "    )\n",
    "    biases = tf.get_variable(\n",
    "        name='final_layer_biases',\n",
    "        shape=(NUM_CLASSES),\n",
    "        initializer=tf.random_normal_initializer(seed=2020)\n",
    "    )\n",
    "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "    labels_one_hot = tf.one_hot(\n",
    "        indices=self._labels,\n",
    "        depth=NUM_CLASSES,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=labels_one_hot,\n",
    "        logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    predicted_labels = tf.argmax(probs, axis=1)\n",
    "    predicted_labels = tf.squeeze(predicted_labels)\n",
    "    return predicted_labels, loss\n",
    "\n",
    "  def trainer(self, loss, learning_rate):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return train_op\n",
    "\n",
    "#train and compute accuracy\n",
    "def train_and_evaluate_RNN(lstm_size, batch_size):\n",
    "  acc=[]\n",
    "  with open('/content/drive/My Drive/Data_Colab/vocab-raw.txt',encoding='latin-1') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "  train_data_reader = DataReader(\n",
    "    data=train_data, \n",
    "    labels=train_labels, \n",
    "    sentence_lengths=train_sentence_lengths\n",
    "  )\n",
    "  test_data_reader = DataReader(\n",
    "    data=test_data, \n",
    "    labels=test_labels, \n",
    "    sentence_lengths=test_sentence_lengths\n",
    "  )\n",
    "  tf.reset_default_graph()\n",
    "  tf.set_random_seed(2020)\n",
    "  rnn = RNN(\n",
    "      vocab_size=vocab_size,\n",
    "      embedding_size=300,\n",
    "      lstm_size=lstm_size,\n",
    "      batch_size=batch_size\n",
    "  )\n",
    "  predicted_labels, loss = rnn.build_graph()\n",
    "  train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    step = 0\n",
    "    MAX_STEP = 1e6\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "      next_train_batch = train_data_reader.next_batch(batch_size)\n",
    "      data, labels, sentence_lengths = next_train_batch\n",
    "      plabels_eval, loss_eval, _ = sess.run(\n",
    "          [predicted_labels, loss, train_op],\n",
    "          feed_dict={\n",
    "              rnn._data: data,\n",
    "              rnn._labels: labels,\n",
    "              rnn._sentence_lengths: sentence_lengths,\n",
    "          }\n",
    "      )\n",
    "      step += 1\n",
    "      # if step % 20 == 0:\n",
    "      #   print('loss:', loss_eval)\n",
    "      if train_data_reader._batch_id == 0:\n",
    "        num_true_preds = 0\n",
    "        while True:\n",
    "          next_test_batch = test_data_reader.next_batch(batch_size)\n",
    "          data, labels, sentence_lengths = next_test_batch\n",
    "          test_plabels_eval = sess.run(\n",
    "              predicted_labels,\n",
    "              feed_dict={\n",
    "                  rnn._data: data,\n",
    "                  rnn._labels: labels,\n",
    "                  rnn._sentence_lengths: sentence_lengths,\n",
    "              }\n",
    "          )\n",
    "          matches = np.equal(test_plabels_eval, labels)\n",
    "          num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "          if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "        print('Epoch:', train_data_reader._num_epoch)\n",
    "        print('Accuracy on test data:', num_true_preds*100./len(test_data_reader._data))\n",
    "        acc.append(num_true_preds*100./len(test_data_reader._data))\n",
    "  return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

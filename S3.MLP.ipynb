{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca71dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anh Minh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e653ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n",
    "        \n",
    "        weights_1 = tf.get_variable(\n",
    "            name = 'weight_input_hidden',\n",
    "            shape = (self._vocab_size, self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed=2022),\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name = 'biases_input_hidden',\n",
    "            shape = (self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed=2022),\n",
    "        )\n",
    "        weights_2 = tf.get_variable(\n",
    "            name = \"weights_hidden_output\",\n",
    "            shape = (self._hidden_size, NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed=2022),\n",
    "        )\n",
    "        biases_2 = tf.get_variable(\n",
    "            name = \"biases_hidden_output\",\n",
    "            shape = (NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed=2022),\n",
    "        )\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "    \n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6896504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for line in d_lines:\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split(\"<fff>\")\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(\":\")[0]), float(token.split(\":\")[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = min(start + self._batch_size, len(self._data))\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end == len(self._data):\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2022)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e36660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = \",\".join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = \"\\n\".join(\n",
    "            [\n",
    "                \",\".join([str(number) for number in value[row]])\n",
    "                for row in range(value.shape[0])\n",
    "            ]\n",
    "        )\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"w\") as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f7d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(vocab_size):\n",
    "    train_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-train-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    test_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-test-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07598ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open(os.getcwd()+\"/saved-paras/\"+filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d60be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anh Minh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step: 0, loss: 9.414592742919922\n",
      "step: 1, loss: 0.6328482031822205\n",
      "step: 2, loss: 0.0005247645312920213\n",
      "step: 3, loss: 2.7298485747451195e-06\n",
      "step: 4, loss: 8.344546245098172e-07\n",
      "step: 5, loss: 0.0\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 14.02230453491211\n",
      "step: 10, loss: 33.304203033447266\n",
      "step: 11, loss: 29.2415828704834\n",
      "step: 12, loss: 24.001264572143555\n",
      "step: 13, loss: 17.916475296020508\n",
      "step: 14, loss: 11.774836540222168\n",
      "step: 15, loss: 5.9914116859436035\n",
      "step: 16, loss: 0.9426317811012268\n",
      "step: 17, loss: 0.12705358862876892\n",
      "step: 18, loss: 0.0004097946221008897\n",
      "step: 19, loss: 3.824170107691316e-06\n",
      "step: 20, loss: 1.1682499234666466e-07\n",
      "step: 21, loss: 18.593366622924805\n",
      "step: 22, loss: 25.262248992919922\n",
      "step: 23, loss: 22.324996948242188\n",
      "step: 24, loss: 19.371986389160156\n",
      "step: 25, loss: 16.421585083007812\n",
      "step: 26, loss: 13.393752098083496\n",
      "step: 27, loss: 10.621005058288574\n",
      "step: 28, loss: 8.022726058959961\n",
      "step: 29, loss: 5.583151340484619\n",
      "step: 30, loss: 3.1986348628997803\n",
      "step: 31, loss: 1.4693593978881836\n",
      "step: 32, loss: 0.40436238050460815\n",
      "step: 33, loss: 7.928461074829102\n",
      "step: 34, loss: 8.407604217529297\n",
      "step: 35, loss: 7.908675670623779\n",
      "step: 36, loss: 6.91670036315918\n",
      "step: 37, loss: 5.943951606750488\n",
      "step: 38, loss: 4.309691905975342\n",
      "step: 39, loss: 2.9816598892211914\n",
      "step: 40, loss: 1.8301901817321777\n",
      "step: 41, loss: 1.0562288761138916\n",
      "step: 42, loss: 0.5922074317932129\n",
      "step: 43, loss: 0.25351664423942566\n",
      "step: 44, loss: 1.4611687660217285\n",
      "step: 45, loss: 13.633111953735352\n",
      "step: 46, loss: 12.900395393371582\n",
      "step: 47, loss: 12.523275375366211\n",
      "step: 48, loss: 11.585309982299805\n",
      "step: 49, loss: 10.948616981506348\n",
      "step: 50, loss: 9.898734092712402\n",
      "step: 51, loss: 8.845536231994629\n",
      "step: 52, loss: 7.770610332489014\n",
      "step: 53, loss: 6.774628162384033\n",
      "step: 54, loss: 5.884741306304932\n",
      "step: 55, loss: 5.063879489898682\n",
      "step: 56, loss: 8.730572700500488\n",
      "step: 57, loss: 11.108882904052734\n",
      "step: 58, loss: 10.774633407592773\n",
      "step: 59, loss: 9.807256698608398\n",
      "step: 60, loss: 8.37894058227539\n",
      "step: 61, loss: 7.791848182678223\n",
      "step: 62, loss: 7.058935165405273\n",
      "step: 63, loss: 6.387533187866211\n",
      "step: 64, loss: 5.7174601554870605\n",
      "step: 65, loss: 5.303301811218262\n",
      "step: 66, loss: 4.7991623878479\n",
      "step: 67, loss: 4.065191745758057\n",
      "step: 68, loss: 6.823993682861328\n",
      "step: 69, loss: 7.342601299285889\n",
      "step: 70, loss: 7.2708420753479\n",
      "step: 71, loss: 6.314950942993164\n",
      "step: 72, loss: 5.965348720550537\n",
      "step: 73, loss: 5.443075656890869\n",
      "step: 74, loss: 5.19129753112793\n",
      "step: 75, loss: 4.83182430267334\n",
      "step: 76, loss: 4.463297367095947\n",
      "step: 77, loss: 4.216640949249268\n",
      "step: 78, loss: 3.8832461833953857\n",
      "step: 79, loss: 3.5570385456085205\n",
      "step: 80, loss: 7.03060245513916\n",
      "step: 81, loss: 6.882256031036377\n",
      "step: 82, loss: 6.661292552947998\n",
      "step: 83, loss: 6.420236110687256\n",
      "step: 84, loss: 6.094369411468506\n",
      "step: 85, loss: 5.820120811462402\n",
      "step: 86, loss: 5.636624813079834\n",
      "step: 87, loss: 5.263410568237305\n",
      "step: 88, loss: 5.096094131469727\n",
      "step: 89, loss: 4.549559116363525\n",
      "step: 90, loss: 4.397355079650879\n",
      "step: 91, loss: 4.367734909057617\n",
      "step: 92, loss: 8.604646682739258\n",
      "step: 93, loss: 8.249468803405762\n",
      "step: 94, loss: 7.977724552154541\n",
      "step: 95, loss: 7.653421401977539\n",
      "step: 96, loss: 7.483358860015869\n",
      "step: 97, loss: 7.059130668640137\n",
      "step: 98, loss: 6.834341526031494\n",
      "step: 99, loss: 6.514404773712158\n",
      "step: 100, loss: 6.120473861694336\n",
      "step: 101, loss: 5.736323356628418\n",
      "step: 102, loss: 5.267397880554199\n",
      "step: 103, loss: 5.333343505859375\n",
      "step: 104, loss: 9.127608299255371\n",
      "step: 105, loss: 8.887887001037598\n",
      "step: 106, loss: 8.316408157348633\n",
      "step: 107, loss: 8.168767929077148\n",
      "step: 108, loss: 7.754322528839111\n",
      "step: 109, loss: 7.3816986083984375\n",
      "step: 110, loss: 6.946107387542725\n",
      "step: 111, loss: 6.702845573425293\n",
      "step: 112, loss: 6.120261192321777\n",
      "step: 113, loss: 5.878568649291992\n",
      "step: 114, loss: 5.4467878341674805\n",
      "step: 115, loss: 5.3859663009643555\n",
      "step: 116, loss: 8.501599311828613\n",
      "step: 117, loss: 8.092811584472656\n",
      "step: 118, loss: 7.784985542297363\n",
      "step: 119, loss: 7.438020706176758\n",
      "step: 120, loss: 7.392543315887451\n",
      "step: 121, loss: 6.871473789215088\n",
      "step: 122, loss: 6.460352897644043\n",
      "step: 123, loss: 6.191658973693848\n",
      "step: 124, loss: 5.901130199432373\n",
      "step: 125, loss: 5.577941417694092\n",
      "step: 126, loss: 5.300037384033203\n",
      "step: 127, loss: 5.322168350219727\n",
      "step: 128, loss: 6.177385330200195\n",
      "step: 129, loss: 5.9625725746154785\n",
      "step: 130, loss: 5.749362945556641\n",
      "step: 131, loss: 5.443623065948486\n",
      "step: 132, loss: 5.137819766998291\n",
      "step: 133, loss: 4.828629970550537\n",
      "step: 134, loss: 4.436097145080566\n",
      "step: 135, loss: 3.9147109985351562\n",
      "step: 136, loss: 3.6428885459899902\n",
      "step: 137, loss: 3.250419855117798\n",
      "step: 138, loss: 2.688986301422119\n",
      "step: 139, loss: 3.4261391162872314\n",
      "step: 140, loss: 6.395059108734131\n",
      "step: 141, loss: 6.2428364753723145\n",
      "step: 142, loss: 5.852382183074951\n",
      "step: 143, loss: 5.536384105682373\n",
      "step: 144, loss: 5.161494255065918\n",
      "step: 145, loss: 4.84273624420166\n",
      "step: 146, loss: 4.5830864906311035\n",
      "step: 147, loss: 4.260711193084717\n",
      "step: 148, loss: 4.045478343963623\n",
      "step: 149, loss: 3.7635140419006348\n",
      "step: 150, loss: 3.6022276878356934\n",
      "step: 151, loss: 5.417842864990234\n",
      "step: 152, loss: 7.547321796417236\n",
      "step: 153, loss: 7.2376933097839355\n",
      "step: 154, loss: 7.165487289428711\n",
      "step: 155, loss: 6.789007663726807\n",
      "step: 156, loss: 6.516491889953613\n",
      "step: 157, loss: 6.269687652587891\n",
      "step: 158, loss: 5.945281982421875\n",
      "step: 159, loss: 5.622471809387207\n",
      "step: 160, loss: 5.290726184844971\n",
      "step: 161, loss: 5.000678539276123\n",
      "step: 162, loss: 4.626355171203613\n",
      "step: 163, loss: 6.4956440925598145\n",
      "step: 164, loss: 7.85910701751709\n",
      "step: 165, loss: 7.580930233001709\n",
      "step: 166, loss: 7.207240581512451\n",
      "step: 167, loss: 7.0281291007995605\n",
      "step: 168, loss: 6.70374870300293\n",
      "step: 169, loss: 6.419452667236328\n",
      "step: 170, loss: 6.189736366271973\n",
      "step: 171, loss: 5.862805366516113\n",
      "step: 172, loss: 5.519809722900391\n",
      "step: 173, loss: 5.219632625579834\n",
      "step: 174, loss: 4.886713981628418\n",
      "step: 175, loss: 8.81160831451416\n",
      "step: 176, loss: 9.502819061279297\n",
      "step: 177, loss: 8.848193168640137\n",
      "step: 178, loss: 8.382171630859375\n",
      "step: 179, loss: 8.278690338134766\n",
      "step: 180, loss: 7.718020439147949\n",
      "step: 181, loss: 7.3389787673950195\n",
      "step: 182, loss: 7.054599761962891\n",
      "step: 183, loss: 6.757042407989502\n",
      "step: 184, loss: 6.439996242523193\n",
      "step: 185, loss: 6.1274871826171875\n",
      "step: 186, loss: 5.8192596435546875\n",
      "step: 187, loss: 8.780230522155762\n",
      "step: 188, loss: 9.587491989135742\n",
      "step: 189, loss: 9.355164527893066\n",
      "step: 190, loss: 9.155617713928223\n",
      "step: 191, loss: 8.825952529907227\n",
      "step: 192, loss: 8.562997817993164\n",
      "step: 193, loss: 8.27881145477295\n",
      "step: 194, loss: 7.9667510986328125\n",
      "step: 195, loss: 7.66305685043335\n",
      "step: 196, loss: 7.36883544921875\n",
      "step: 197, loss: 7.058417320251465\n",
      "step: 198, loss: 7.701473236083984\n",
      "step: 199, loss: 7.7152886390686035\n",
      "step: 200, loss: 7.515530586242676\n",
      "step: 201, loss: 7.240190505981445\n",
      "step: 202, loss: 6.974966526031494\n",
      "step: 203, loss: 6.679537296295166\n",
      "step: 204, loss: 6.393881797790527\n",
      "step: 205, loss: 6.091765880584717\n",
      "step: 206, loss: 5.6293158531188965\n",
      "step: 207, loss: 5.133788585662842\n",
      "step: 208, loss: 4.620149612426758\n",
      "step: 209, loss: 7.376214981079102\n",
      "step: 210, loss: 9.910039901733398\n",
      "step: 211, loss: 9.858294486999512\n",
      "step: 212, loss: 9.478747367858887\n",
      "step: 213, loss: 9.173625946044922\n",
      "step: 214, loss: 8.982385635375977\n",
      "step: 215, loss: 8.503515243530273\n",
      "step: 216, loss: 8.173213958740234\n",
      "step: 217, loss: 7.8542985916137695\n",
      "step: 218, loss: 6.613113880157471\n",
      "step: 219, loss: 5.597278594970703\n",
      "step: 220, loss: 5.122910022735596\n",
      "step: 221, loss: 4.775692462921143\n",
      "step: 222, loss: 4.39628267288208\n",
      "step: 223, loss: 4.069382190704346\n",
      "step: 224, loss: 3.5694730281829834\n",
      "step: 225, loss: 3.249220848083496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 226, loss: 2.474700450897217\n",
      "step: 227, loss: 2.760728359222412\n",
      "step: 228, loss: 2.6415648460388184\n",
      "step: 229, loss: 2.7318222522735596\n",
      "step: 230, loss: 2.7153613567352295\n",
      "step: 231, loss: 2.7346057891845703\n",
      "step: 232, loss: 2.499502658843994\n",
      "step: 233, loss: 2.7657365798950195\n",
      "step: 234, loss: 2.756166458129883\n",
      "step: 235, loss: 2.939451217651367\n",
      "step: 236, loss: 2.7604901790618896\n",
      "step: 237, loss: 2.7466864585876465\n",
      "step: 238, loss: 2.5600271224975586\n",
      "step: 239, loss: 2.766481637954712\n",
      "step: 240, loss: 2.663541316986084\n",
      "step: 241, loss: 2.802391290664673\n",
      "step: 242, loss: 2.522230386734009\n",
      "step: 243, loss: 2.617781162261963\n",
      "step: 244, loss: 2.4638259410858154\n",
      "step: 245, loss: 2.728048801422119\n",
      "step: 246, loss: 2.51198410987854\n",
      "step: 247, loss: 2.2539498805999756\n",
      "step: 248, loss: 2.4254581928253174\n",
      "step: 249, loss: 2.1723697185516357\n",
      "step: 250, loss: 2.2241404056549072\n",
      "step: 251, loss: 1.8455015420913696\n",
      "step: 252, loss: 2.2493278980255127\n",
      "step: 253, loss: 1.9516651630401611\n",
      "step: 254, loss: 2.3166117668151855\n",
      "step: 255, loss: 1.9944071769714355\n",
      "step: 256, loss: 2.183107852935791\n",
      "step: 257, loss: 1.7718462944030762\n",
      "step: 258, loss: 1.8190340995788574\n",
      "step: 259, loss: 1.9422013759613037\n",
      "step: 260, loss: 1.9812947511672974\n",
      "step: 261, loss: 1.7100352048873901\n",
      "step: 262, loss: 1.6927266120910645\n",
      "step: 263, loss: 1.4906460046768188\n",
      "step: 264, loss: 1.8771388530731201\n",
      "step: 265, loss: 1.512009620666504\n",
      "step: 266, loss: 1.6867704391479492\n",
      "step: 267, loss: 2.1325552463531494\n",
      "step: 268, loss: 1.4595483541488647\n",
      "step: 269, loss: 1.732216238975525\n",
      "step: 270, loss: 1.782106637954712\n",
      "step: 271, loss: 1.766243577003479\n",
      "step: 272, loss: 1.2858794927597046\n",
      "step: 273, loss: 1.8844964504241943\n",
      "step: 274, loss: 1.4533978700637817\n",
      "step: 275, loss: 1.4092332124710083\n",
      "step: 276, loss: 1.7011735439300537\n",
      "step: 277, loss: 1.4283467531204224\n",
      "step: 278, loss: 1.4026449918746948\n",
      "step: 279, loss: 1.5865789651870728\n",
      "step: 280, loss: 1.6891212463378906\n",
      "step: 281, loss: 1.135416865348816\n",
      "step: 282, loss: 1.3825443983078003\n",
      "step: 283, loss: 1.1915186643600464\n",
      "step: 284, loss: 1.2754919528961182\n",
      "step: 285, loss: 1.1753021478652954\n",
      "step: 286, loss: 1.2807953357696533\n",
      "step: 287, loss: 1.0986133813858032\n",
      "step: 288, loss: 1.374046802520752\n",
      "step: 289, loss: 1.3808375597000122\n",
      "step: 290, loss: 1.4682683944702148\n",
      "step: 291, loss: 1.2382261753082275\n",
      "step: 292, loss: 1.3323253393173218\n",
      "step: 293, loss: 1.0578278303146362\n",
      "step: 294, loss: 1.3173816204071045\n",
      "step: 295, loss: 1.2241603136062622\n",
      "step: 296, loss: 1.2290667295455933\n",
      "step: 297, loss: 1.2207293510437012\n",
      "step: 298, loss: 1.347237229347229\n",
      "step: 299, loss: 0.8873863816261292\n",
      "step: 300, loss: 0.9352824687957764\n",
      "step: 301, loss: 1.3660244941711426\n",
      "step: 302, loss: 1.308402419090271\n",
      "step: 303, loss: 1.1089370250701904\n",
      "step: 304, loss: 1.2597723007202148\n",
      "step: 305, loss: 0.8580728769302368\n",
      "step: 306, loss: 0.9038560390472412\n",
      "step: 307, loss: 0.9490571022033691\n",
      "step: 308, loss: 1.0845378637313843\n",
      "step: 309, loss: 1.1368706226348877\n",
      "step: 310, loss: 0.659058690071106\n",
      "step: 311, loss: 1.0309885740280151\n",
      "step: 312, loss: 0.8086371421813965\n",
      "step: 313, loss: 1.023153305053711\n",
      "step: 314, loss: 0.910520613193512\n",
      "step: 315, loss: 0.6473731398582458\n",
      "step: 316, loss: 1.0935771465301514\n",
      "step: 317, loss: 0.7067679762840271\n",
      "step: 318, loss: 0.816647469997406\n",
      "step: 319, loss: 1.010751485824585\n",
      "step: 320, loss: 1.022391676902771\n",
      "step: 321, loss: 0.788130521774292\n",
      "step: 322, loss: 0.9804630279541016\n",
      "step: 323, loss: 0.8210214376449585\n",
      "step: 324, loss: 0.6393086314201355\n",
      "step: 325, loss: 0.7651599049568176\n",
      "step: 326, loss: 0.8603251576423645\n",
      "step: 327, loss: 1.081881046295166\n",
      "step: 328, loss: 0.8723534345626831\n",
      "step: 329, loss: 0.6848384141921997\n",
      "step: 330, loss: 0.8207017779350281\n",
      "step: 331, loss: 0.8286146521568298\n",
      "step: 332, loss: 0.6571059226989746\n",
      "step: 333, loss: 0.8882617950439453\n",
      "step: 334, loss: 0.7761625051498413\n",
      "step: 335, loss: 0.39171963930130005\n",
      "step: 336, loss: 0.8318479061126709\n",
      "step: 337, loss: 0.9011180996894836\n",
      "step: 338, loss: 0.935936689376831\n",
      "step: 339, loss: 0.7193781137466431\n",
      "step: 340, loss: 0.646980881690979\n",
      "step: 341, loss: 0.5621349215507507\n",
      "step: 342, loss: 0.7299889326095581\n",
      "step: 343, loss: 0.4994565546512604\n",
      "step: 344, loss: 0.4544740617275238\n",
      "step: 345, loss: 0.6876693964004517\n",
      "step: 346, loss: 0.33217155933380127\n",
      "step: 347, loss: 0.4012205898761749\n",
      "step: 348, loss: 0.7244714498519897\n",
      "step: 349, loss: 0.7190696001052856\n",
      "step: 350, loss: 1.0365937948226929\n",
      "step: 351, loss: 0.7107136249542236\n",
      "step: 352, loss: 0.6191949844360352\n",
      "step: 353, loss: 0.5819743275642395\n",
      "step: 354, loss: 1.013941764831543\n",
      "step: 355, loss: 0.6730940341949463\n",
      "step: 356, loss: 0.741020143032074\n",
      "step: 357, loss: 0.7933013439178467\n",
      "step: 358, loss: 0.635998010635376\n",
      "step: 359, loss: 1.1102386713027954\n",
      "step: 360, loss: 0.5255275964736938\n",
      "step: 361, loss: 0.6287417411804199\n",
      "step: 362, loss: 0.8214552402496338\n",
      "step: 363, loss: 0.5295087099075317\n",
      "step: 364, loss: 0.686622142791748\n",
      "step: 365, loss: 0.6382268667221069\n",
      "step: 366, loss: 0.6257801055908203\n",
      "step: 367, loss: 0.5123813152313232\n",
      "step: 368, loss: 0.6969850659370422\n",
      "step: 369, loss: 0.6954866051673889\n",
      "step: 370, loss: 0.405924916267395\n",
      "step: 371, loss: 0.43886128067970276\n",
      "step: 372, loss: 0.6896517276763916\n",
      "step: 373, loss: 0.5486303567886353\n",
      "step: 374, loss: 0.6691760420799255\n",
      "step: 375, loss: 0.41291552782058716\n",
      "step: 376, loss: 0.5075851678848267\n",
      "step: 377, loss: 0.6274220943450928\n",
      "step: 378, loss: 0.31544333696365356\n",
      "step: 379, loss: 0.7860817909240723\n",
      "step: 380, loss: 0.8111650943756104\n",
      "step: 381, loss: 0.7183480858802795\n",
      "step: 382, loss: 0.4872039556503296\n",
      "step: 383, loss: 0.4416272044181824\n",
      "step: 384, loss: 0.8337773084640503\n",
      "step: 385, loss: 0.6352572441101074\n",
      "step: 386, loss: 0.31978023052215576\n",
      "step: 387, loss: 0.6511736512184143\n",
      "step: 388, loss: 0.682392954826355\n",
      "step: 389, loss: 0.5241852402687073\n",
      "step: 390, loss: 0.853318452835083\n",
      "step: 391, loss: 0.6936687231063843\n",
      "step: 392, loss: 0.8157991766929626\n",
      "step: 393, loss: 0.5050740242004395\n",
      "step: 394, loss: 0.6099615693092346\n",
      "step: 395, loss: 0.48952651023864746\n",
      "step: 396, loss: 0.641296923160553\n",
      "step: 397, loss: 0.43307921290397644\n",
      "step: 398, loss: 0.5987762212753296\n",
      "step: 399, loss: 0.4639642834663391\n",
      "step: 400, loss: 0.5121983885765076\n",
      "step: 401, loss: 0.6667687892913818\n",
      "step: 402, loss: 0.49848851561546326\n",
      "step: 403, loss: 0.4702441394329071\n",
      "step: 404, loss: 0.2690330445766449\n",
      "step: 405, loss: 0.6609387397766113\n",
      "step: 406, loss: 0.5201122164726257\n",
      "step: 407, loss: 0.4871838390827179\n",
      "step: 408, loss: 0.5079554319381714\n",
      "step: 409, loss: 0.6532882452011108\n",
      "step: 410, loss: 0.6979680061340332\n",
      "step: 411, loss: 0.4844571650028229\n",
      "step: 412, loss: 0.4776427447795868\n",
      "step: 413, loss: 0.47608691453933716\n",
      "step: 414, loss: 0.3602522313594818\n",
      "step: 415, loss: 0.47154149413108826\n",
      "step: 416, loss: 0.4142013490200043\n",
      "step: 417, loss: 0.2457663118839264\n",
      "step: 418, loss: 0.8798538446426392\n",
      "step: 419, loss: 0.3811005651950836\n",
      "step: 420, loss: 0.594204306602478\n",
      "step: 421, loss: 0.2959284484386444\n",
      "step: 422, loss: 0.3177584707736969\n",
      "step: 423, loss: 0.32386061549186707\n",
      "step: 424, loss: 0.46152281761169434\n",
      "step: 425, loss: 0.4206225574016571\n",
      "step: 426, loss: 0.3154571056365967\n",
      "step: 427, loss: 0.6166840195655823\n",
      "step: 428, loss: 0.6656522154808044\n",
      "step: 429, loss: 0.19257313013076782\n",
      "step: 430, loss: 0.5044867396354675\n",
      "step: 431, loss: 0.4353199303150177\n",
      "step: 432, loss: 0.15449032187461853\n",
      "step: 433, loss: 0.3059019446372986\n",
      "step: 434, loss: 0.3572065234184265\n",
      "step: 435, loss: 0.520495593547821\n",
      "step: 436, loss: 0.4521608352661133\n",
      "step: 437, loss: 0.23909986019134521\n",
      "step: 438, loss: 0.6249247193336487\n",
      "step: 439, loss: 0.9690195322036743\n",
      "step: 440, loss: 0.5663082599639893\n",
      "step: 441, loss: 0.26744258403778076\n",
      "step: 442, loss: 0.7403917908668518\n",
      "step: 443, loss: 0.49326038360595703\n",
      "step: 444, loss: 0.33575761318206787\n",
      "step: 445, loss: 0.4397704303264618\n",
      "step: 446, loss: 0.31276583671569824\n",
      "step: 447, loss: 0.4305080473423004\n",
      "step: 448, loss: 0.6157228350639343\n",
      "step: 449, loss: 0.28817346692085266\n",
      "step: 450, loss: 0.7237457036972046\n",
      "step: 451, loss: 0.5199940204620361\n",
      "step: 452, loss: 0.46244117617607117\n",
      "step: 453, loss: 0.06212228536605835\n",
      "step: 454, loss: 0.1270792931318283\n",
      "step: 455, loss: 0.10684329271316528\n",
      "step: 456, loss: 0.32332566380500793\n",
      "step: 457, loss: 0.24685317277908325\n",
      "step: 458, loss: 0.2776121199131012\n",
      "step: 459, loss: 0.30206796526908875\n",
      "step: 460, loss: 0.20335744321346283\n",
      "step: 461, loss: 0.20460011065006256\n",
      "step: 462, loss: 0.21371856331825256\n",
      "step: 463, loss: 0.132863849401474\n",
      "step: 464, loss: 0.09995924681425095\n",
      "step: 465, loss: 0.1497294008731842\n",
      "step: 466, loss: 0.10785875469446182\n",
      "step: 467, loss: 0.14769834280014038\n",
      "step: 468, loss: 0.09259138256311417\n",
      "step: 469, loss: 0.14209981262683868\n",
      "step: 470, loss: 0.2917649745941162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 471, loss: 0.39694905281066895\n",
      "step: 472, loss: 0.27493569254875183\n",
      "step: 473, loss: 0.151875302195549\n",
      "step: 474, loss: 0.2205466330051422\n",
      "step: 475, loss: 0.16700798273086548\n",
      "step: 476, loss: 0.12803015112876892\n",
      "step: 477, loss: 0.21930532157421112\n",
      "step: 478, loss: 0.061292730271816254\n",
      "step: 479, loss: 0.2259766161441803\n",
      "step: 480, loss: 0.2562943398952484\n",
      "step: 481, loss: 0.2500527501106262\n",
      "step: 482, loss: 0.10544415563344955\n",
      "step: 483, loss: 0.35060936212539673\n",
      "step: 484, loss: 0.4285593330860138\n",
      "step: 485, loss: 0.15079961717128754\n",
      "step: 486, loss: 0.2637934386730194\n",
      "step: 487, loss: 0.026384472846984863\n",
      "step: 488, loss: 0.2554023861885071\n",
      "step: 489, loss: 0.1544821858406067\n",
      "step: 490, loss: 0.32247376441955566\n",
      "step: 491, loss: 0.19524602591991425\n",
      "step: 492, loss: 0.053897857666015625\n",
      "step: 493, loss: 0.10523629188537598\n",
      "step: 494, loss: 0.24517516791820526\n",
      "step: 495, loss: 0.12078118324279785\n",
      "step: 496, loss: 0.3094165623188019\n",
      "step: 497, loss: 0.16061297059059143\n",
      "step: 498, loss: 0.25665104389190674\n",
      "step: 499, loss: 0.26147162914276123\n",
      "Epoch: 2\n",
      "Accuracy on test data: 0.7563728093467871\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES =20\n",
    "\n",
    "# Load features size\n",
    "with open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "# mlp initialization\n",
    "mlp = MLP(vocab_size=vocab_size, hidden_size=50)\n",
    "\n",
    "# build graph model\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "\n",
    "# optimizator\n",
    "train_op = mlp.trainer(loss=loss, learning_rate=0.1)\n",
    "\n",
    "# open a session\n",
    "with tf.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset(vocab_size)\n",
    "    \n",
    "    # run session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    steps = 500\n",
    "    # training\n",
    "    for step in range(steps):\n",
    "        # load batch\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        \n",
    "        # feeding\n",
    "        labels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # results\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "        \n",
    "        # save params before train each epouch\n",
    "        if train_data_reader._batch_id == 0:\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_parameters(\n",
    "                name = variable.name,\n",
    "                value = variable.eval(),\n",
    "                epoch = train_data_reader._num_epoch\n",
    "            )\n",
    "\n",
    "# load saved params\n",
    "with tf.Session() as sess:\n",
    "    # load final epoch params\n",
    "    epoch = train_data_reader._num_epoch\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "\n",
    "    # load one epoch to evaluate\n",
    "    num_true_preds = 0\n",
    "    while True:  \n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_labels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict = {\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_labels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch:', epoch)\n",
    "    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
